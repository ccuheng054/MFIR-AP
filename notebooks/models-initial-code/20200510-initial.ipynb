{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "BASE_DIR = os.path.join(\"..\", \"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, TimeDistributed, Activation\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import GRU as RNN\n",
    "\n",
    "EMBEDDING_UNITS = 1024//8\n",
    "N_CLASSES = 2\n",
    "\n",
    "FLOATX='float32'\n",
    "tf.keras.backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras API\n",
    "We will be using Keras in tensorflow to create our models. Keras has two APIs for creating models: sequential and functional. Since our models will have multiple inputs and outputs, we will be using functional API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding TPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See build_TPA_embedding in Conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build embeddings for TPA with view-id \"dummy\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'TPAdummy_input:0' shape=(None, None, 32, 32, 1) dtype=float32>, <tf.Tensor 'TPAdummy_dense/Identity:0' shape=(None, None, 128) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "dummy_input, dummy_output = build_TPA_embedding(\"dummy\")\n",
    "print([dummy_input, dummy_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's connect input and output to create a Keras model and see its summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.engine.training.Model object at 0x7f5af016f278>\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "TPAdummy_input (InputLayer)  [(None, None, 32, 32, 1)] 0         \n",
      "_________________________________________________________________\n",
      "TPAdummy_b1c1 (TimeDistribut (None, None, 32, 32, 64)  640       \n",
      "_________________________________________________________________\n",
      "TPAdummy_b1c2 (TimeDistribut (None, None, 32, 32, 64)  36928     \n",
      "_________________________________________________________________\n",
      "TPAdummy_b1c3 (TimeDistribut (None, None, 32, 32, 64)  36928     \n",
      "_________________________________________________________________\n",
      "TPAdummy_b2m1 (TimeDistribut (None, None, 16, 16, 64)  0         \n",
      "_________________________________________________________________\n",
      "TPAdummy_b2c1 (TimeDistribut (None, None, 16, 16, 128) 73856     \n",
      "_________________________________________________________________\n",
      "TPAdummy_b2c2 (TimeDistribut (None, None, 16, 16, 128) 147584    \n",
      "_________________________________________________________________\n",
      "TPAdummy_b2c3 (TimeDistribut (None, None, 16, 16, 128) 147584    \n",
      "_________________________________________________________________\n",
      "TPAdummy_b3m1 (TimeDistribut (None, None, 8, 8, 128)   0         \n",
      "_________________________________________________________________\n",
      "TPAdummy_flat (TimeDistribut (None, None, 8192)        0         \n",
      "_________________________________________________________________\n",
      "TPAdummy_dense (TimeDistribu (None, None, 128)         1048704   \n",
      "=================================================================\n",
      "Total params: 1,492,224\n",
      "Trainable params: 1,492,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Model(dummy_input, dummy_output)\n",
    "print(model)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now build 3 embeddings for TPAs: TPA1, TPA2, TPA3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPA_model_ids = [1, 2, 3]\n",
    "io_TPAs = [build_TPA_embedding(id) for id in TPA_model_ids]\n",
    "io_TPA1, io_TPA2, io_TPA3 = io_TPAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"TPA1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "TPA1_input (InputLayer)      [(None, None, 32, 32, 1)] 0         \n",
      "_________________________________________________________________\n",
      "TPA1_b1c1 (TimeDistributed)  (None, None, 32, 32, 64)  640       \n",
      "_________________________________________________________________\n",
      "TPA1_b1c2 (TimeDistributed)  (None, None, 32, 32, 64)  36928     \n",
      "_________________________________________________________________\n",
      "TPA1_b1c3 (TimeDistributed)  (None, None, 32, 32, 64)  36928     \n",
      "_________________________________________________________________\n",
      "TPA1_b2m1 (TimeDistributed)  (None, None, 16, 16, 64)  0         \n",
      "_________________________________________________________________\n",
      "TPA1_b2c1 (TimeDistributed)  (None, None, 16, 16, 128) 73856     \n",
      "_________________________________________________________________\n",
      "TPA1_b2c2 (TimeDistributed)  (None, None, 16, 16, 128) 147584    \n",
      "_________________________________________________________________\n",
      "TPA1_b2c3 (TimeDistributed)  (None, None, 16, 16, 128) 147584    \n",
      "_________________________________________________________________\n",
      "TPA1_b3m1 (TimeDistributed)  (None, None, 8, 8, 128)   0         \n",
      "_________________________________________________________________\n",
      "TPA1_flat (TimeDistributed)  (None, None, 8192)        0         \n",
      "_________________________________________________________________\n",
      "TPA1_dense (TimeDistributed) (None, None, 128)         1048704   \n",
      "=================================================================\n",
      "Total params: 1,492,224\n",
      "Trainable params: 1,492,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Model(*io_TPA1, name=\"TPA1\").summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging TPA embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first merge 3 TPA embeddings by concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_TPAs = [x[0] for x in io_TPAs]\n",
    "o_TPAs = [x[1] for x in io_TPAs]\n",
    "TPA_merged = Concatenate(name='view_concat', axis=-1)([*o_TPAs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(EMBEDDING_UNITS*len(io_TPAs), activation='tanh', recurrent_activation='sigmoid', return_sequences=True, name = \"TPA_GRU\")(TPA_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPA_dense = TimeDistributed(Dense(N_CLASSES, activation=None), name=\"TPA_dense\")(rnn)\n",
    "TPA_classification = Activation(activation='sigmoid', name='TPA_classification')(TPA_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Model(i_TPAs, TPA_classification, name=\"Model_3xTPA\")\n",
    "print(classifier.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss\n",
    "We will start with EL introudced by Jain, et al. but keep in mind that FRAMES, and FRAME_SHIFT parameters of our training will vary between models. We will implement class Losses_Keras that will be initialized in each training with parameters to pass to loss functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAMES_N = 100\n",
    "FRAME_SHIFT_N = 0\n",
    "losses = Losses_Keras(frames = FRAMES_N, frame_shift = FRAME_SHIFT_N)\n",
    "exponential_loss = losses.get_exponential_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(exponential_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(loss=exponential_loss, optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 50, 2)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FRAMES_N = 50\n",
    "FRAME_SHIFT_N = 0\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "H, W = 32, 32\n",
    "FRAMES = FRAMES_N\n",
    "BATCH_SHAPE = (BATCH_SIZE, FRAMES, H, W, 1)\n",
    "FRAME_BATCH_SHAPE = (BATCH_SIZE, H, W, 1)\n",
    "\n",
    "data = [np.random.rand(np.prod(BATCH_SHAPE)).astype(np.float32).reshape(BATCH_SHAPE) for i in range(3)]\n",
    "y_pos = ((np.random.rand(BATCH_SIZE * FRAMES).reshape([BATCH_SIZE, FRAMES]) > 0.5) * 1)\n",
    "y_neg = ((1 - y_pos) > 0.5) * 1\n",
    "y_true = np.stack([y_neg, y_pos], axis=-1)\n",
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_3xTPA\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "TPA1_input (InputLayer)         [(None, None, 32, 32 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "TPA2_input (InputLayer)         [(None, None, 32, 32 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "TPA3_input (InputLayer)         [(None, None, 32, 32 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "TPA1_b1c1 (TimeDistributed)     (None, None, 32, 32, 640         TPA1_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "TPA2_b1c1 (TimeDistributed)     (None, None, 32, 32, 640         TPA2_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "TPA3_b1c1 (TimeDistributed)     (None, None, 32, 32, 640         TPA3_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "TPA1_b1c2 (TimeDistributed)     (None, None, 32, 32, 36928       TPA1_b1c1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "TPA2_b1c2 (TimeDistributed)     (None, None, 32, 32, 36928       TPA2_b1c1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "TPA3_b1c2 (TimeDistributed)     (None, None, 32, 32, 36928       TPA3_b1c1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "TPA1_b1c3 (TimeDistributed)     (None, None, 32, 32, 36928       TPA1_b1c2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "TPA2_b1c3 (TimeDistributed)     (None, None, 32, 32, 36928       TPA2_b1c2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "TPA3_b1c3 (TimeDistributed)     (None, None, 32, 32, 36928       TPA3_b1c2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "TPA1_b2m1 (TimeDistributed)     (None, None, 16, 16, 0           TPA1_b1c3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "TPA2_b2m1 (TimeDistributed)     (None, None, 16, 16, 0           TPA2_b1c3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "TPA3_b2m1 (TimeDistributed)     (None, None, 16, 16, 0           TPA3_b1c3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "TPA1_b2c1 (TimeDistributed)     (None, None, 16, 16, 73856       TPA1_b2m1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "TPA2_b2c1 (TimeDistributed)     (None, None, 16, 16, 73856       TPA2_b2m1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "TPA3_b2c1 (TimeDistributed)     (None, None, 16, 16, 73856       TPA3_b2m1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "TPA1_b2c2 (TimeDistributed)     (None, None, 16, 16, 147584      TPA1_b2c1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "TPA2_b2c2 (TimeDistributed)     (None, None, 16, 16, 147584      TPA2_b2c1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "TPA3_b2c2 (TimeDistributed)     (None, None, 16, 16, 147584      TPA3_b2c1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "TPA1_b2c3 (TimeDistributed)     (None, None, 16, 16, 147584      TPA1_b2c2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "TPA2_b2c3 (TimeDistributed)     (None, None, 16, 16, 147584      TPA2_b2c2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "TPA3_b2c3 (TimeDistributed)     (None, None, 16, 16, 147584      TPA3_b2c2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "TPA1_b3m1 (TimeDistributed)     (None, None, 8, 8, 1 0           TPA1_b2c3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "TPA2_b3m1 (TimeDistributed)     (None, None, 8, 8, 1 0           TPA2_b2c3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "TPA3_b3m1 (TimeDistributed)     (None, None, 8, 8, 1 0           TPA3_b2c3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "TPA1_flat (TimeDistributed)     (None, None, 8192)   0           TPA1_b3m1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "TPA2_flat (TimeDistributed)     (None, None, 8192)   0           TPA2_b3m1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "TPA3_flat (TimeDistributed)     (None, None, 8192)   0           TPA3_b3m1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "TPA1_dense (TimeDistributed)    (None, None, 128)    1048704     TPA1_flat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "TPA2_dense (TimeDistributed)    (None, None, 128)    1048704     TPA2_flat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "TPA3_dense (TimeDistributed)    (None, None, 128)    1048704     TPA3_flat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "view_concat (Concatenate)       (None, None, 384)    0           TPA1_dense[0][0]                 \n",
      "                                                                 TPA2_dense[0][0]                 \n",
      "                                                                 TPA3_dense[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "TPA_GRU (GRU)                   (None, None, 384)    887040      view_concat[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "TPA_dense (TimeDistributed)     (None, None, 2)      770         TPA_GRU[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "TPA_classification (Activation) (None, None, 2)      0           TPA_dense[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,364,482\n",
      "Trainable params: 5,364,482\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "losses = Losses_Keras(frames = FRAMES_N, frame_shift = FRAME_SHIFT_N)\n",
    "exponential_loss = losses.get_exponential_loss()\n",
    "TPA_model_ids = [1, 2, 3]\n",
    "io_TPAs = [build_TPA_embedding(id) for id in TPA_model_ids]\n",
    "io_TPA1, io_TPA2, io_TPA3 = io_TPAs\n",
    "i_TPAs = [x[0] for x in io_TPAs]\n",
    "o_TPAs = [x[1] for x in io_TPAs]\n",
    "TPA_merged = Concatenate(name='view_concat', axis=-1)([*o_TPAs])\n",
    "rnn = RNN(EMBEDDING_UNITS*len(io_TPAs), activation='tanh', recurrent_activation='sigmoid', return_sequences=True, name = \"TPA_GRU\")(TPA_merged)\n",
    "TPA_dense = TimeDistributed(Dense(N_CLASSES, activation=None), name=\"TPA_dense\")(rnn)\n",
    "TPA_classification = Activation(activation='sigmoid', name='TPA_classification')(TPA_dense)\n",
    "classifier = Model(i_TPAs, TPA_classification, name=\"Model_3xTPA\")\n",
    "classifier.compile(loss=exponential_loss, optimizer=\"adam\", metrics=['accuracy'])\n",
    "print(classifier.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32 samples\n",
      "32/32 [==============================] - 5s 145ms/sample - loss: 3974.3440 - accuracy: 0.5044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5a4419a860>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(data, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, TimeDistributed, Activation\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import GRU as RNN\n",
    "\n",
    "EMBEDDING_UNITS = 1024//8\n",
    "N_CLASSES = 2\n",
    "\n",
    "FLOATX='float32'\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def build_TPA_embedding(view_id):\n",
    "    # VGG-16 but dims are scaled by 1/7, only 3 blocks\n",
    "    # FUTURE Think about filters -> skipping cncnts\n",
    "    # https://towardsdatascience.com/step-by-step-vgg16-implementation-in-keras-for-beginners-a833c686ae6c\n",
    "    # b=block c=conv m=maxpool\n",
    "    # input>b1c1>b1c2>b1c3>b2m1>b2c1>b2c2>b2c3>b3m1>flatten>fc\n",
    "    embedding_input = Input(shape=(None, 32, 32, 1), name='TPA{}_input'.format(view_id))\n",
    "    # block1\n",
    "    b1c1 = TimeDistributed(Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"), name='TPA{}_b1c1'.format(view_id))(embedding_input)\n",
    "    b1c2 = TimeDistributed(Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"), name='TPA{}_b1c2'.format(view_id))(b1c1)\n",
    "    b1c3 = TimeDistributed(Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"), name='TPA{}_b1c3'.format(view_id))(b1c2)\n",
    "    # block2\n",
    "    b2m1 = TimeDistributed(MaxPool2D(pool_size=(2, 2), strides=(2, 2)), name='TPA{}_b2m1'.format(view_id))(b1c3)\n",
    "    b2c1 = TimeDistributed(Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"), name='TPA{}_b2c1'.format(view_id))(b2m1)\n",
    "    b2c2 = TimeDistributed(Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"), name='TPA{}_b2c2'.format(view_id))(b2c1)\n",
    "    b2c3 = TimeDistributed(Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"), name='TPA{}_b2c3'.format(view_id))(b2c2)\n",
    "    # block3\n",
    "    b3m1 = TimeDistributed(MaxPool2D(pool_size=(2, 2), strides=(2, 2)), name='TPA{}_b3m1'.format(view_id))(b2c3)\n",
    "    # FC\n",
    "    flat = TimeDistributed(Flatten(), name='TPA{}_flat'.format(view_id))(b3m1)\n",
    "    dense = TimeDistributed(Dense(units=EMBEDDING_UNITS, activation=\"relu\"), name='TPA{}_dense'.format(view_id))(flat) # flatten/fc = 6.125\n",
    "    embedding_output = dense\n",
    "    return embedding_input, embedding_output\n",
    "\n",
    "class Losses_Keras:\n",
    "    def __init__(self, frames, frame_shift):\n",
    "        self.frames = frames\n",
    "        self.frame_shfit = frame_shift\n",
    "    \n",
    "    def get_exponential_loss(self, from_logits=False):\n",
    "        def exponential_loss(y_true, y_pred, from_logits=from_logits):\n",
    "            # [B, F, 2], [B, F, 2]\n",
    "            # TODO ADD JAIN\n",
    "            # L_p = sigma_t(-exp())\n",
    "            # L_n = softmax_cross_entropy\n",
    "            # EL = L_p + L_n\n",
    "            # https://stackoverflow.com/questions/39192380/tensorflow-one-class-classification\n",
    "            # https://github.com/keras-team/keras/blob/7a39b6c62d43c25472b2c2476bd2a8983ae4f682/keras/backend/cntk_backend.py#L1065\n",
    "            if not tf.is_tensor(y_pred):\n",
    "                y_pred = tf.constant(y_pred, dtype=FLOATX)\n",
    "            y_true = tf.cast(y_true, y_pred.dtype)\n",
    "            if from_logits:\n",
    "                y_pred = tf.keras.activations.sigmoid(y_pred)\n",
    "            y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1. - tf.keras.backend.epsilon())\n",
    "            log_pos = (y_true)*tf.math.log(y_pred)\n",
    "            log_neg = (1.0 - y_true)*tf.math.log(1.0 - y_pred)\n",
    "            # ONLY IF THE ACTION HAPPENS AT THE LAST FRAME\n",
    "            Y = tf.cast((tf.shape(y_true)[-1]), FLOATX)\n",
    "            k = tf.cast(self.frames/5, FLOATX)\n",
    "            # XXX 20?\n",
    "            arg=tf.cast(tf.math.exp(-(Y-tf.range(Y)-1)/k), FLOATX)\n",
    "            positive_loss = -tf.reduce_sum(tf.broadcast_to(tf.math.exp(arg), tf.shape(log_pos)) * log_pos)\n",
    "            negative_loss = -tf.reduce_sum(log_neg)\n",
    "            total_loss = positive_loss + negative_loss\n",
    "            return total_loss\n",
    "        return exponential_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[0,1],[1,0], [0,1]], tf.int32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0 1]\n",
      " [1 0]\n",
      " [0 1]], shape=(3, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[0,1],[1,0], [0,1]], tf.int32) \n",
    "a = tf.expand_dims(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0 1]]\n",
      "\n",
      " [[1 0]]\n",
      "\n",
      " [[0 1]]], shape=(3, 1, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[0,1],[1,0], [0,1]], tf.int32) \n",
    "a = tf.expand_dims(a, 1)\n",
    "a = tf.tile(a,tf.constant([1, 50, 1], tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]]\n",
      "\n",
      " [[1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]]\n",
      "\n",
      " [[0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]]], shape=(3, 50, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
